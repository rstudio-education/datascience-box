---
title: "Linear models with a single predictor <br> `r emo::ji('dango')`"
author: ""
output:
  xaringan::moon_reader:
    css: "../slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r child = "../setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(scales)
library(broom)
library(here)
```



class: middle

# Interpreting models

---

## The data

```{r load-pp, message=FALSE}
pp <- read_csv(
  "data/paris-paintings.csv", 
  na = c("n/a", "", "NA")
  )
```

---

## Height & width

.small[
```{r}
m_ht_wt <- lm(Height_in ~ Width_in, data = pp)
tidy(m_ht_wt)
```
]

--

$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$

--
- **Slope:** For each additional inch the painting is wider, the height is 
expected to be higher, on average, by 0.78 inches.

--
- **Intercept:** Paintings that are 0 inches wide are expected to be 3.62 
inches high, on average. The intercept is meaningless in the context of these 
data, it only serves to adjust the height of the line.

---

## Linear model with a single predictor

- We're interested in $\beta_0$ (population parameter for the intercept) and 
$\beta_1$ (population parameter for the slope) in the following model:

$$ \hat{y} = \beta_0 + \beta_1~x $$

--
- But we don't have access to the population, so we use sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$

--
- Another way of describing this model is

$$y = b_0 + b_1~x + e $$

---

## Least squares regression

- The regression line minimizes the sum of squared residuals.

--
- If $e_i = y - \hat{y}$, then, the regression line minimizes 
$\sum_{i = 1}^n e_i^2$.

---

## Properties of the least squares regression line

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$: $(\bar{x}, \bar{y})$:  
$$\hat{y} = b_0 + b_1 x ~ \rightarrow ~ b_0 = \hat{y} - b_1 x$$

--
- The slope has the same sign as the correlation coefficient: $b_1 = r \frac{s_y}{s_x}$

--
- The sum of the residuals is zero: $\sum_{i = 1}^n e_i = 0$

--
- The residuals and $x$ values are uncorrelated.

---

## Height & landscape features

```{r}
m_ht_lands <- lm(Height_in ~ factor(landsALL), data = pp)
tidy(m_ht_lands)
```

--

$$\widehat{Height_{in}} = 22.68 - 5.65~landsALL$$

---

## Height & landscape features (cont.)

- **Slope:** Paintings with landscape features are expected, on average,
to be 5.65 inches shorter than paintings that without landscape features.
    - Compares baseline level (`landsALL = 0`) to other level
    (`landsALL = 1`).
- **Intercept:** Paintings that don't have landscape features are expected, on 
average, to be 22.68 inches tall.

---

## Categorical predictor with 2 levels

```{r echo=FALSE}
pp %>% 
  select(name, price, landsALL) %>% 
  slice(1:8)
```

---

## Relationship between height and school

```{r}
m_ht_sch <- lm(Height_in ~ school_pntg, data = pp)
tidy(m_ht_sch)
```

--

.question[
Where did all these new predictors come from?
]

---

## Categorical explanatory variables with 3+ levels

- When the categorical explanatory variable has many levels, they're encoded to
**dummy variables**.
- Each coefficient describes the expected difference between heights in that 
particular school compared to the baseline level.

.midi[
```{r echo=FALSE}
tidy(m_ht_sch)
```
]

--

.question[
What is the baseline level?
]

---

.question[
`landsALL` had two levels: 0 and 1. What was the baseline level for the model with `landsALL` as predictor?
]

```{r echo=FALSE}
tidy(m_ht_lands)
```

---

.question[
`school` has 7 levels: A = Austrian, D/FL = Dutch/Flemish, F = French, G = German, I = Italian, S = Spanish, X = Unknown. What us the baseline level for the model with `school` as predictor?
]

```{r echo=FALSE}
tidy(m_ht_sch)
``` 

---

## Dummy coding

```{r echo=FALSE}
pp %>% 
  select(school_pntg) %>% 
  group_by(school_pntg) %>% 
  sample_n(1) %>%
  mutate(
    D_FL = as.integer(ifelse(school_pntg == "D/FL", 1L, 0)),
    F    = as.integer(ifelse(school_pntg == "F", 1L, 0)),
    G    = as.integer(ifelse(school_pntg == "G", 1L, 0)),
    I    = as.integer(ifelse(school_pntg == "I", 1L, 0)),
    S    = as.integer(ifelse(school_pntg == "S", 1L, 0)),
    X    = as.integer(ifelse(school_pntg == "X", 1L, 0))
  )
```

---

## Interpretation of coefficients

.question[
Interpret the intercept, slope for Dutch/Flemish, slope for Spanish paintings.
]

```{r echo=FALSE}
tidy(m_ht_sch)
``` 

---

## The linear model with multiple predictors

- Population model:

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k $$

--

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

---

## Correlation does not imply causation!

Remember this when interpreting model coefficients

<br>

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/cell_phones.png")
```

.footnote[
Source: XKCD, [Cell phones](https://xkcd.com/925/)
]

---

class: middle

# Prediction with models

---

## Predict height from width

.question[
On average, how tall are paintings that are 60 inches wide?  
$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$
]

--

```{r}
3.62 + 0.78 * 60
```

--

"On average, we expect paintings that are 60 inches wide to be 50.42 inches high."

--

**Warning:** We "expect" this to happen, but there will be some variability. 
(We'll learn about measuring the variability around the prediction later.)

---

## Prediction in R

Use the `predict()` function to make a prediction:

- First argument: model
- Second argument: data frame representing the new observation(s) for which 
you want to make a prediction, with variables with the same names as those 
to build the model, except for the response variables

---

## Prediction in R (cont.)

- Making a prediction for a single new observation:
```{r oredict-single-new}
newpainting <- tibble(Width_in = 60)
predict(m_ht_wt, newdata = newpainting)
```

- Making a prediction for multiple new observations:
```{r oredict-multiple-new}
newpaintings <- tibble(Width_in = c(50, 60, 70))
predict(m_ht_wt, newdata = newpaintings)
```

---

## Prediction vs. extrapolation

.question[
On average, how tall are paintings that are 400 inches wide?
$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$
]

```{r extrapolate, warning = FALSE, echo=FALSE, fig.height=2, fig.width=5}
newdata <- tibble(Width_in = 400)
newdata <- newdata %>%
  mutate(Height_in = predict(m_ht_wt, newdata = newdata))

ggplot(data = pp, aes(x = Width_in, y = Height_in)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, color = "#A7D5E8", se = FALSE) +
  xlim(0, 420) +
  ylim(0, 320) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = 0, xend = Width_in, yend = Height_in), color = "#1E5C65", lty = 2) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = Height_in, xend = 0, yend = Height_in), color = "#1E5C65", lty = 2)
```

---

## Watch out for extrapolation!

> "When those blizzards hit the East Coast this winter, it proved to my satisfaction 
that global warming was a fraud. That snow was freezing cold. But in an alarming 
trend, temperatures this spring have risen. Consider this: On February 6th it was 10 
degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So 
clearly folks the climate debate rages on."<sup>1</sup>  <br>
Stephen Colbert, April 6th, 2010

.footnote[
[1] OpenIntro Statistics. "Extrapolation is treacherous." OpenIntro Statistics.
]

---

class: middle

# Measuring model fit

---

## Measuring the strength of the fit

- The strength of the fit of a linear model is most commonly evaluated using $R^2$.
- It tells us what percent of variability in the response variable is explained by 
the model.
- The remainder of the variability is explained by variables not included in the 
model.

---

## Height vs. lanscape features

.question[
Which of the following is the correct interpretation of $R^2$ of the model 
below.
]

.small[
```{r}
m_ht_lands <- lm(Height_in ~ factor(landsALL), data = pp)
glance(m_ht_lands)$r.squared
```
]

**(a)** 3.5% of the variability in heights of painting can be explained by 
whether or not the painting has landscape features.

**(b)** 3.5% of the variability in whether a painting has landscape features 
can be explained by heights of paintings.

**(c)** This model predicts 3.5% of heights of paintings accurately.

**(d)** This model predicts 3.5% of whether a painting has landscape features 
accurately.

**(e)** 3.5% of the time there is an association between whether a painting 
has landscape features and its height.

---

## Height vs. width

.small[
```{r}
glance(m_ht_wt)
glance(m_ht_wt)$r.squared # extract R-squared
```
]

Roughly 68% of the variability in heights of paintings can be explained by their
widths.

---

class: middle

# Tidy regression output

---

## Not-so-tidy regression output

- You might come accross these in your googling adventures, but we'll try to stay away from them
- Not because they are wrong, but because they don't result in tidy data frames as results.

---

## Not-so-tidy regression output (1)

#### Option 1:

```{r}
m_ht_wt
```

---

## Not-so-tidy regression output (2)

#### Option 2:

```{r}
summary(m_ht_wt)
```

---

## Review

.question[
What makes a data frame tidy?
]

--
- Each variable forms a column.
- Each observation forms a row.
- Each type of observational unit forms a table.

---

## Tidy regression output

Achieved with functions from the broom package:

- `tidy`: Constructs a data frame that summarizes the model's statistical findings: coefficient estimates, *standard errors, test statistics, p-values*.
- `glance`: Constructs a concise one-row summary of the model. This typically contains values such as $R^2$, adjusted $R^2$, *and residual standard error that are computed once for the entire model*.
- `augment`: Adds columns to the original data that was modeled. This includes predictions and residuals.

---

## We've already seen...

- Tidy your model's statistical findings

.small[
```{r}
tidy(m_ht_wt)
```
]

- Glance to assess model fit

.small[
```{r}
glance(m_ht_wt)
```
]

---

## Augment data with model results

New variables of note (for now):

- `.fitted`: Predicted value of the response variable
- `.resid`: Residuals

.small[
```{r}
augment(m_ht_wt) %>%
  slice(1:5)
```
]

--

.question[
Why might we be interested in these new variables?
]

---

## Residuals plot

.small[
```{r fig.height=2, fig.width=5}
m_ht_wt_aug <- augment(m_ht_wt)
ggplot(m_ht_wt_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted height", y = "Residuals")
```
]

---

class: middle

# Model checking

---

## "Linear" models

- We're fitting a "linear" model, which assumes a linear relationship between 
our explanatory and response variables.
- But how do we assess this?

---

## Looking for...

- Residuals distributed randomly around 0
- With no visible pattern along the x or y axes

```{r fig.height=2, fig.width=5, echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_resid = rnorm(1000, mean = 0, sd = 30),
  fake_predicted = runif(1000, min = 0, max = 200)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

### Fan shapes

```{r fig.height=2, fig.width=5, echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_resid = c(rnorm(100, mean = 0, sd = 1), 
                 rnorm(100, mean = 0, sd = 15), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 20), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 50), 
                 rnorm(100, mean = 0, sd = 35), 
                 rnorm(100, mean = 0, sd = 40),
                 rnorm(200, mean = 0, sd = 80)),
  fake_predicted = seq(0.2, 200, 0.2)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

### Residuals correlated with predicted values

```{r fig.height=2, fig.width=5, echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = c(
    rnorm(500, mean = -20, sd = 10),
    rnorm(500, mean = 10, sd = 10)
  )
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

### Groups of patterns

```{r fig.height=2, fig.width=5, echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = fake_predicted + rnorm(1000, mean = 0, sd = 50)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

### Any patterns!

```{r fig.height=2, fig.width=5, echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(-100, 100, 0.4),
  fake_resid = -5*fake_predicted^2 - 3*fake_predicted + 20000 + rnorm(501, mean = 0, sd = 10000)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted", y = "Residuals")
```

---

.question[
What patterns does the residuals plot reveal that should make us question 
whether a linear model is a good fit for modeling the relationship 
between height and width of paintings?
]

```{r fig.height=2, fig.width=5, echo=FALSE}
ggplot(m_ht_wt_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted height", y = "Residuals")
```
