<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data science ethics   üîç</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: middle, inverse, title-slide

# Data science ethics <br> üîç
### 

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="https://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---



## Data science ethics

- Misrepresentation
- Privacy
- Algorithmic bias

---

class: middle

# Misrepresentation

---

.question[
What is the difference between these two pictures? Which presents a better way to represent these data?
]

&lt;br&gt;

&lt;img src="img/axis-start-at-0.png" width="850" style="display: block; margin: auto;" /&gt;

.footnote[
Ingraham, C. (2019) ["You‚Äôve been reading charts wrong. Here‚Äôs how a pro does it."](https://www.washingtonpost.com/business/2019/10/14/youve-been-reading-charts-wrong-heres-how-pro-does-it/), The Washington Post, 14 Oct.
]

---

.question[
What is wrong with this picture? How would you correct it?
]

&lt;img src="img/cost_of_gas.png" width="850" style="display: block; margin: auto;" /&gt;

---

.question[
Do you recognize this map? What does it show?
]

&lt;img src="img/election-2016-county.png" width="700" style="display: block; margin: auto;" /&gt;

--

.footnote[
Gamio, L. (2016) ["Election maps are telling you big lies about small things"](https://www.washingtonpost.com/graphics/politics/2016-election/how-election-maps-lie/), The Washington Post, 1 Nov.
]

---

.pull-left[
&lt;img src="img/citizens-for-trump.png" width="350" style="display: block; margin: auto;" /&gt;
]
--
.pull-right[
&lt;img src="img/counties-for-trump.png" width="350" style="display: block; margin: auto;" /&gt;
]

.footnote[
Credit: Alberto Cairo, [Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
]

---

&lt;img src="img/cairo-vote-percentages.png" width="800" style="display: block; margin: auto;" /&gt;

.footnote[
Credit: Alberto Cairo, [Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
]

---

&lt;img src="img/cairo-what-matters.png" width="900" style="display: block; margin: auto;" /&gt;

.footnote[
Credit: Alberto Cairo, [Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
]

---

## Further reading

.pull-left[
&lt;img src="img/cairo-how-charts-lie.jpg" width="350" style="display: block; margin: auto 0 auto auto;" /&gt;
]
.pull-right[
[How Charts Lie](https://wwnorton.com/books/9781324001560)  
Getting Smarter about Visual Information  
by Alberto Cairo
]

---

class: middle

# Privacy

---

class: middle

### Ok Cupid

---

## OK Cupid data breach

- In 2016, researchers published data of 70,000 OkCupid users‚Äîincluding usernames, political leanings, drug usage, and intimate sexual details.

&gt;Some may object to the ethics of gathering and releasing this data. However, all the data found in the dataset are or were already publicly available, so releasing this dataset merely presents it in a more useful form.  
&gt;Researchers Emil Kirkegaard and Julius Daugbjerg Bjerrek√¶r

- Although the researchers did not release the real names and pictures of the OkCupid users, critics noted that their identities could easily be uncovered from the details provided‚Äîsuch as from the usernames.

---

.question[
In analysis of data individuals willingly shared publicly on a given platform (e.g. social media data), how do you make sure you don't violate reasonable expectations of privacy?
]

![Tweet stating OK Cupid data was already public](img/okcupid-tweet.png)



---

class: middle

### Facebook &amp; Cambridge Analytica

---

![Diagram explaining what happened in the Fecebook / Cambridge Analytica scandal](img/facebook-cambridge-analytica-scandal-explained-the-guardian-graphic.jpg)

---

class: middle

# Algorithmic bias

---

class: middle

### First a bit of fun...
### The Hathaway Effect

---

.pull-left[
&lt;img src="img/hathaway-effect-poster.jpg" width="350" style="display: block; margin: auto 0 auto auto;" /&gt;
]
.pull-right[
**Oct. 3, 2008:** Rachel Getting Married opens: BRK.A up 0.44%  
**Jan. 5, 2009:** Bride Wars opens: BRK.A up 2.61%  
**Feb. 8, 2010:** Valentine‚Äôs Day opens: BRK.A up 1.01%  
**March 5, 2010:** Alice in Wonderland opens:  BRK.A up 0.74%  
**Nov. 24, 2010:** Love and Other Drugs opens: BRK.A up 1.62%  
**Nov. 29, 2010:** Anne announced as co-host of the Oscars: BRK.A up 0.25%
]

.footnote[
Mirvish, D. (2011) [The Hathaway Effect: How Anne Gives Warren Buffett a Rise](https://www.huffpost.com/entry/the-hathaway-effect-how-a_b_830041), The Huffington Post, 2 Mar.
]

---

class: middle

### Algorithmic bias and gender

---

## Google Translate

&lt;img src="img/google-translate-gender-bias.png" width="900" style="display: block; margin: auto;" /&gt;

---

## Amazon's experimental hiring algorithm

- Used AI to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon, some of the people said
- Company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way
- Amazon‚Äôs system taught itself that male candidates were preferable

&gt;Gender bias was not the only issue. Problems with the data that underpinned the models‚Äô judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said.

.footnote[
Dastin, J. (2018) [Amazon scraps secret AI recruiting tool that showed bias against women](https://reut.rs/2Od9fPr), Reuters, 10 Oct.
]

---

class: middle

### Algorithmic bias and race

---

.pull-left[
&lt;img src="img/guardian-facial-recognition.png" width="500" style="display: block; margin: auto 0 auto auto;" /&gt;
]
.pull-right[
['A white mask worked better': why algorithms are not colour blind](https://www.theguardian.com/technology/2017/may/28/joy-buolamwini-when-algorithms-are-racist-facial-recognition-bias)  
&lt;br&gt;
*by Ian Tucker*
]

---

## Further watching

&lt;div style="max-width:854px"&gt;&lt;div style="position:relative;height:0;padding-bottom:56.25%"&gt;&lt;iframe src="https://embed.ted.com/talks/lang/en/joy_buolamwini_how_i_m_fighting_bias_in_algorithms" width="854" height="480" style="position:absolute;left:0;top:0;width:100%;height:100%" frameborder="0" scrolling="no" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;

---

## Criminal Sentencing

![](img/propublica-criminal-sentencing.png)

There‚Äôs software used across the country to predict future criminals. And it‚Äôs biased against blacks.
.small[
[propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), May 23, 2016
]

---

## A tale of two convicts

.pull-left[
![](img/propublica-prater-broden-1.png)
]
--
.pull-right[
![](img/propublica-prater-broden-2.png)
]

---

class: middle

&gt;‚ÄúAlthough these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice,‚Äù he said, adding, ‚Äúthey may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society.‚Äù
&gt;  
&gt;Then U.S. Attorney General Eric Holder (2014)

---

## ProPublica analysis

### Data: 

Risk scores assigned to more than 7,000 people arrested in Broward County, Florida, in 2013 and 2014 + whether they were charged with new crimes over the next two years

---

## ProPublica analysis

### Results:

- 20% of those predicted to commit violent crimes actually did
- Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors)
![](img/propublica-results.png)
- Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants
- White defendants were mislabeled as low risk more often than black defendants

---

## Further reading

.pull-left[
&lt;img src="img/propublica-machine-bias.png" width="500" style="display: block; margin: auto 0 auto auto;" /&gt;
]
.pull-right[
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
There‚Äôs software used across the country to predict future criminals. And it‚Äôs biased against blacks.  
&lt;br&gt;
*by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, ProPublica*
]

---

## Further watching

.center[
&lt;iframe width="800" height="450" src="https://www.youtube.com/embed/MfThopD7L1Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;  
Predictive Policing: Bias In, Bias Out
by Kristian Lum
]

---

## How to make a racist AI without trying

.pull-left[
![](img/racist_ai_python.png)
.center[
[Link to post](https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)
]
]
.pull-right[
![](img/racist_ai_r.png)
.center[
[Link to post](https://notstatschat.rbind.io/2018/09/27/how-to-write-a-racist-ai-in-r-without-really-trying/)
]
]

---

## Review

.question[
A company uses a machine learning algorithm to determine which job advertisement to display for users searching for technology jobs. Based on past results, the algorithm tends to display lower paying jobs for women than for men (after controlling for other characteristics than gender).

What ethical considerations might be considered when reviewing this algorithm?
]

.footnote[
Source: Modern Data Science with R, by Baumer, Kaplan, and Horton
]

---

class: middle

# Continuing your education on data science ethics

---

## Further reading

.pull-left[
![](img/ethics-data-science.jpg)
]
.pull-right[
[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)  
by Mike Loukides, Hilary Mason, DJ Patil  
(Free Kindle download)
]

---

## Further reading

.pull-left[
![Cover of the book "Weapons of Math Destruction" by Cathy O'Neil](img/weapons-of-math-destruction.jpg)
]
.pull-right[
[Weapons of Math Destruction](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)  
How Big Data Increases Inequality and Threatens Democracy  
&lt;br&gt;
*by Cathy O'Neil*
]

---

## Further watching

.center[
&lt;iframe width="800" height="450" src="https://www.youtube.com/embed/fgf2VjnhpCs?start=1162" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;  
AI for Good in the R and Python ecosystems  
by Julien Cornebise
]

---

## Parting thoughts

- At some point during your data science learning journey you will learn tools that can be used unethically
- You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)

.question[
How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?
]

---

## Do good with data

- Data Science for Social Good: 
  - [University of Chicago](https://dssg.uchicago.edu/)
  - [The Alan Turing Institute](https://www.turing.ac.uk/collaborate-turing/data-science-social-good)
- [DataKind](https://www.datakind.org/): DataKind brings high-impact organizations together with leading data scientists to use data science in the service of humanity.
- Sign the Manifesto for Data Practices: [datapractices.org/manifesto](https://datapractices.org/manifesto/)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
